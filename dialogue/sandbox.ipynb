{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class DialogSandbox:\n",
    "#     def __init__(self, model_name='gpt2', device='cuda'):\n",
    "#         self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#         self.model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "#         self.device = device\n",
    "\n",
    "#     def generate_dialog(self, prompt, max_length=100, num_return_sequences=1, temperature=1.0, top_p=0.9):\n",
    "#         \"\"\"\n",
    "#         Generate dialog based on a given prompt using GPT-2.\n",
    "        \n",
    "#         :param prompt: Prompt for the dialog generation.\n",
    "#         :param max_length: Maximum length of the generated dialog.\n",
    "#         :param num_return_sequences: Number of dialog sequences to generate.\n",
    "#         :param temperature: Sampling temperature for diversity.\n",
    "#         :param top_p: Nucleus sampling parameter for controlling diversity.\n",
    "#         :return: Generated dialog sequences.\n",
    "#         \"\"\"\n",
    "#         # Prepare the prompt\n",
    "#         input_ids = self.tokenizer.encode(prompt, return_tensors='pt').to(self.device)\n",
    "        \n",
    "#         # Generate sequences\n",
    "#         # chat_outputs = self.model.generate(\n",
    "#         #     input_ids,\n",
    "#         #     max_length=max_length + len(input_ids[0]),\n",
    "#         #     num_return_sequences=num_return_sequences,\n",
    "#         #     temperature=temperature,\n",
    "#         #     top_p=top_p,\n",
    "#         #     pad_token_id=self.tokenizer.eos_token_id\n",
    "#         # )\n",
    "\n",
    "#         chat_outputs = self.model.generate(\n",
    "#             input_ids,\n",
    "#             max_length=max_length + len(input_ids[0]),\n",
    "#             num_return_sequences=num_return_sequences,\n",
    "#             temperature=temperature,\n",
    "#             top_p=top_p,\n",
    "#             pad_token_id=self.tokenizer.eos_token_id,\n",
    "#             do_sample=True  # Ensure sampling is enabled to support num_return_sequences > 1\n",
    "#         )\n",
    "\n",
    "#         # generation_config = self.model.generate(\n",
    "#         #     max_new_tokens=150, \n",
    "#         #     num_return_sequences=100,\n",
    "#         #         do_sample=True, \n",
    "#         #         top_p=0.9, \n",
    "#         #         temperature=1.0, \n",
    "#         #         no_repeat_ngram_size=3,\n",
    "#         #         # bad_words_ids=bad_words_ids,\n",
    "#         #         pad_token_id=self.tokenizer.pad_token_id\n",
    "#         #     )\n",
    "        \n",
    "#         # Decode and return the sequences\n",
    "#         return [self.tokenizer.decode(output, skip_special_tokens=True) for output in chat_outputs]\n",
    "\n",
    "# # Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "#     sandbox = DialogSandbox(model_name='gpt2', device='cpu')  # Use 'cuda' if GPU is available\n",
    "#     prompt = \"The following is a conversation between two old friends, John and Sarah, who unexpectedly meet at a park: John: Hey Sarah! It's been a long time. How have you been?\"\n",
    "#     dialogs = sandbox.generate_dialog(prompt, max_length=50, num_return_sequences=3, temperature=0.8, top_p=0.95)\n",
    "    \n",
    "#     for i, dialog in enumerate(dialogs, 1):\n",
    "#         print(f\"Dialog {i}:\\n{dialog}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 51\u001b[0m\n\u001b[1;32m     47\u001b[0m initial_sentence\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSam: How did Jamie take to their first day at school? Mine was a bit teary-eyed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# prompt = \"The following is a conversation between two old friends, John and Sarah, who unexpectedly meet at a park:\"\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# initial_sentence = \"John: Hey Sarah! It's been a long time. How have you been?\"\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m dialogs \u001b[38;5;241m=\u001b[39m \u001b[43msandbox\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_dialog\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_sentence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.95\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, dialog \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dialogs, \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDialog \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mdialog\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 21\u001b[0m, in \u001b[0;36mDialogSandbox.generate_dialog\u001b[0;34m(self, prompt, initial_sentence, max_length, num_return_sequences, temperature, top_p)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03mGenerate dialog based on a given prompt using GPT-2, excluding the initial prompt from the output.\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03m:return: Generated dialog sequences, starting with the initial sentence.\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Prepare the prompt\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Generate sequences\u001b[39;00m\n\u001b[1;32m     24\u001b[0m chat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m     25\u001b[0m     input_ids,\n\u001b[1;32m     26\u001b[0m     max_length\u001b[38;5;241m=\u001b[39mmax_length \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(input_ids[\u001b[38;5;241m0\u001b[39m]),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m     do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# Ensure sampling is enabled\u001b[39;00m\n\u001b[1;32m     32\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch_envs/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2573\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, return_tensors, **kwargs)\u001b[0m\n\u001b[1;32m   2536\u001b[0m \u001b[38;5;129m@add_end_docstrings\u001b[39m(\n\u001b[1;32m   2537\u001b[0m     ENCODE_KWARGS_DOCSTRING,\n\u001b[1;32m   2538\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2556\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2557\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mint\u001b[39m]:\n\u001b[1;32m   2558\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2559\u001b[0m \u001b[38;5;124;03m    Converts a string to a sequence of ids (integer), using the tokenizer and vocabulary.\u001b[39;00m\n\u001b[1;32m   2560\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2571\u001b[0m \u001b[38;5;124;03m            method).\u001b[39;00m\n\u001b[1;32m   2572\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2573\u001b[0m     encoded_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2574\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2575\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2576\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2577\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2579\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2580\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2581\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2582\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2583\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2585\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m encoded_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch_envs/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2981\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2971\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   2972\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   2973\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   2974\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2978\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2979\u001b[0m )\n\u001b[0;32m-> 2981\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2982\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2983\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2984\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2985\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2986\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2987\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2988\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2989\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2990\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2991\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2992\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2993\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2994\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2995\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2996\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2997\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2998\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2999\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3000\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch_envs/lib/python3.10/site-packages/transformers/models/gpt2/tokenization_gpt2_fast.py:172\u001b[0m, in \u001b[0;36mGPT2TokenizerFast._encode_plus\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m is_split_into_words \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_split_into_words\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_prefix_space \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_split_into_words, (\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou need to instantiate \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with add_prefix_space=True \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto use it with pretokenized inputs.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    170\u001b[0m )\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_plus\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch_envs/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:576\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_encode_plus\u001b[39m(\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    556\u001b[0m     text: Union[TextInput, PreTokenizedInput],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    574\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchEncoding:\n\u001b[1;32m    575\u001b[0m     batched_input \u001b[38;5;241m=\u001b[39m [(text, text_pair)] \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;28;01melse\u001b[39;00m [text]\n\u001b[0;32m--> 576\u001b[0m     batched_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatched_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;66;03m# Return tensor is None, then we can remove the leading batch axis\u001b[39;00m\n\u001b[1;32m    597\u001b[0m     \u001b[38;5;66;03m# Overflowing tokens are returned as a batch of output so we keep them in this case\u001b[39;00m\n\u001b[1;32m    598\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_tensors \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_overflowing_tokens:\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch_envs/lib/python3.10/site-packages/transformers/models/gpt2/tokenization_gpt2_fast.py:162\u001b[0m, in \u001b[0;36mGPT2TokenizerFast._batch_encode_plus\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    156\u001b[0m is_split_into_words \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_split_into_words\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_prefix_space \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_split_into_words, (\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou need to instantiate \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with add_prefix_space=True \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto use it with pretokenized inputs.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m )\n\u001b[0;32m--> 162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch_envs/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:504\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;66;03m# Set the truncation and padding strategy and restore the initial configuration\u001b[39;00m\n\u001b[1;32m    496\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_truncation_and_padding(\n\u001b[1;32m    497\u001b[0m     padding_strategy\u001b[38;5;241m=\u001b[39mpadding_strategy,\n\u001b[1;32m    498\u001b[0m     truncation_strategy\u001b[38;5;241m=\u001b[39mtruncation_strategy,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    501\u001b[0m     pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[1;32m    502\u001b[0m )\n\u001b[0;32m--> 504\u001b[0m encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_pretokenized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;66;03m# Convert encoding to dict\u001b[39;00m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;66;03m# `Tokens` has type: Tuple[\u001b[39;00m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;66;03m#                       List[Dict[str, List[List[int]]]] or List[Dict[str, 2D-Tensor]],\u001b[39;00m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;66;03m#                       List[EncodingFast]\u001b[39;00m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;66;03m#                    ]\u001b[39;00m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;66;03m# with nested dimensions corresponding to batch, overflows, sequence length\u001b[39;00m\n\u001b[1;32m    516\u001b[0m tokens_and_encodings \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_encoding(\n\u001b[1;32m    518\u001b[0m         encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    527\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m encoding \u001b[38;5;129;01min\u001b[39;00m encodings\n\u001b[1;32m    528\u001b[0m ]\n",
      "\u001b[0;31mTypeError\u001b[0m: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]"
     ]
    }
   ],
   "source": [
    "class DialogSandbox:\n",
    "    def __init__(self, model_name='gpt2', device='cpu'):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "        self.device = device\n",
    "\n",
    "    \n",
    "    def generate_dialog(self, prompt, initial_sentence, max_length=100, num_return_sequences=1, temperature=1.0, top_p=0.9):\n",
    "        \"\"\"\n",
    "        Generate dialog based on a given prompt using GPT-2, excluding the initial prompt from the output.\n",
    "        \n",
    "        :param prompt: Complete prompt for the dialog generation, including instructions.\n",
    "        :param initial_sentence: The initial sentence of the dialogue to include in the output.\n",
    "        :param max_length: Maximum length of the generated dialog.\n",
    "        :param num_return_sequences: Number of dialog sequences to generate.\n",
    "        :param temperature: Sampling temperature for diversity.\n",
    "        :param top_p: Nucleus sampling parameter for controlling diversity.\n",
    "        :return: Generated dialog sequences, starting with the initial sentence.\n",
    "        \"\"\"\n",
    "        # Prepare the prompt\n",
    "        input_ids = self.tokenizer.encode(prompt, return_tensors='pt').to(self.device)\n",
    "\n",
    "        # Generate sequences\n",
    "        chat_outputs = self.model.generate(\n",
    "            input_ids,\n",
    "            max_length=max_length + len(input_ids[0]),\n",
    "            num_return_sequences=num_return_sequences,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            pad_token_id=self.tokenizer.eos_token_id,\n",
    "            do_sample=True  # Ensure sampling is enabled\n",
    "        )\n",
    "        \n",
    "        # Decode the generated sequences and trim the instruction part\n",
    "        generated_dialogs = [self.tokenizer.decode(output, skip_special_tokens=True) for output in chat_outputs]\n",
    "        trimmed_dialogs = [dialog.replace(prompt, initial_sentence, 1) for dialog in generated_dialogs]\n",
    "\n",
    "        return trimmed_dialogs\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    sandbox = DialogSandbox(model_name='gpt2', device='cpu')  # Use 'cuda' if GPU is available\n",
    "    # prompt = \"The following is a conversation between two old friends, John and Sarah, who unexpectedly meet at a park:\"\n",
    "    # initial_sentence = \"John: Hey Sarah! It's been a long time. How have you been?\"\n",
    "    \n",
    "    prompt = \"Here's a chat between two parents discussing their kids' first day at school:\",\n",
    "    initial_sentence= \"Sam: How did Jamie take to their first day at school? Mine was a bit teary-eyed.\"\n",
    "\n",
    "    # prompt = \"The following is a conversation between two old friends, John and Sarah, who unexpectedly meet at a park:\"\n",
    "    # initial_sentence = \"John: Hey Sarah! It's been a long time. How have you been?\"\n",
    "    dialogs = sandbox.generate_dialog(prompt, initial_sentence, max_length=100, num_return_sequences=3, temperature=0.8, top_p=0.95)\n",
    "    \n",
    "    for i, dialog in enumerate(dialogs, 1):\n",
    "        print(f\"Dialog {i}:\\n{dialog}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dialog 1:\n",
      "Mia: Alex? Is that really you? It's been ages! Mia: Yeah. I think I know you. I'm just thinking about you. I know you're really lonely. It seems so hard to me to just sit on your bed and let you go. Mia: I'm so sorry! I didn't mean to do that to you. But now you're awake and you're telling me this. What was that? I thought I just saw you two go out together. It's like you can't wait to talk to me again.\n",
      "\n",
      "\n",
      "\n",
      "Dialog 2:\n",
      "Mia: Alex? Is that really you? It's been ages!\n",
      "\n",
      "Alex: I'm sorry.\n",
      "\n",
      "Mia: So if you don't mind, just stay there until you get your luggage and a beer. We'll be there! Alex: You'll be there too.\n",
      "\n",
      "Mia: I'm happy we're not in this together.\n",
      "\n",
      "Alex: You know, I'm pretty much my new boyfriend, after all, because I'm so happy and excited to be out with you.\n",
      "\n",
      "Mia: So it's okay\n",
      "\n",
      "Dialog 3:\n",
      "Mia: Alex? Is that really you? It's been ages! Alex: I'm just glad you're okay. Mia: Oh! That was really weird. I'm really glad you're okay. You're doing well. You're making friends, and I'm glad you're feeling better. I really miss you. But I'm not too sure what to do. I don't know what to do. I'd love to go see you again. Mia: No, you're not the only one! Alex: I'm not too sure. I think we\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class DialogSandbox:\n",
    "    def __init__(self, model_name='gpt2', device='cpu'):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "        self.device = device\n",
    "\n",
    "    \n",
    "    def generate_dialog(self, prompt, initial_sentence, max_length=100, num_return_sequences=1, temperature=1.0, top_p=0.9):\n",
    "        \"\"\"\n",
    "        Generate dialog based on a given prompt using GPT-2, excluding the initial prompt from the output.\n",
    "        \n",
    "        :param prompt: Complete prompt for the dialog generation, including instructions.\n",
    "        :param initial_sentence: The initial sentence of the dialogue to include in the output.\n",
    "        :param max_length: Maximum length of the generated dialog.\n",
    "        :param num_return_sequences: Number of dialog sequences to generate.\n",
    "        :param temperature: Sampling temperature for diversity.\n",
    "        :param top_p: Nucleus sampling parameter for controlling diversity.\n",
    "        :return: Generated dialog sequences, starting with the initial sentence.\n",
    "        \"\"\"\n",
    "        full_prompt = \" \"+ prompt+ \" \" +initial_sentence\n",
    "\n",
    "        # Prepare the prompt\n",
    "        input_ids = self.tokenizer.encode(full_prompt, return_tensors='pt').to(self.device)\n",
    "\n",
    "        # Generate sequences\n",
    "        chat_outputs = self.model.generate(\n",
    "            input_ids,\n",
    "            max_length=max_length + len(input_ids[0]),\n",
    "            num_return_sequences=num_return_sequences,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            pad_token_id=self.tokenizer.eos_token_id,\n",
    "            do_sample=True  # Ensure sampling is enabled\n",
    "        )\n",
    "        \n",
    "        # Decode the generated sequences and trim the instruction part\n",
    "        generated_dialogs = [self.tokenizer.decode(output, skip_special_tokens=True) for output in chat_outputs]\n",
    "        trimmed_dialogs = [dialog.replace(full_prompt, initial_sentence, 1) for dialog in generated_dialogs]\n",
    "\n",
    "        return trimmed_dialogs\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sandbox = DialogSandbox(model_name='gpt2', device='cpu')  # Use 'cuda' if GPU is available\n",
    "    # prompt = \"The following is a conversation between two old friends, John and Sarah, who unexpectedly meet at a park:\"\n",
    "    # initial_sentence = \"John: Hey Sarah! It's been a long time. How have you been?\"\n",
    "\n",
    "    prompt = \"This is a conversation between two college roommates, Mia and Alex, who haven't seen each other in years and unexpectedly bump into each other at an airport.\"\n",
    "    initial_sentence = \"Mia: Alex? Is that really you? It's been ages!\"\n",
    "\n",
    "    # prompt = \"A chat between siblings planning a surprise birthday party for their mother. They are discussing who will do what for the party preparations.\"\n",
    "    # initial_sentence = \"Chris: So, have we decided who's going to distract mom on the day of the party?\"\n",
    "\n",
    "    # prompt = \"The following is a conversation between friends discussing their opinions on a new blockbuster movie they just watched together.\"\n",
    "    # initial_sentence = \"Jordan: That movie was incredible, wasn't it? What did you think?\"\n",
    "\n",
    "    dialogs = sandbox.generate_dialog(prompt, initial_sentence, max_length=100, num_return_sequences=3, temperature=0.8, top_p=0.95)\n",
    "    \n",
    "    for i, dialog in enumerate(dialogs, 1):\n",
    "        print(f\"Dialog {i}:\\n{dialog}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jamie: Ever been rock climbing? Looking for a buddy to try it with. We're here for you. We'll be throwing this weekend in a remote, rural location just off of the main road.\n",
      "\n",
      "\n",
      "Bicycle rentals at this location are also available for rent. Please visit our bike rentals page for more information on what rentals we have available.\n",
      "------\n",
      "Drew: In a music rut. Send me your top three songs?\n",
      "\n",
      "Reddy: We love our podcasts and their audience needs some help! They would love some help writing for a podcast if you'd like. Send me your top four song requests for the podcast.\n",
      "\n",
      "Randy: A great list of good song recommendations. Thanks Drew\n",
      "\n",
      "Drew: It's important to find a good listener in a podcast, but that's about it. Here's a video of how they approached making the list\n",
      "\n",
      "Randy: We want a podcast to stand on its own. We're not saying just show people all\n",
      "------\n",
      "Taylor: I just finished reading 'Ocean Depths'. Blew my mind. I really loved the book and it's a great read. I have read many of the books from the book but this one is by far my favorite. I loved it so much and will definitely be back again! - November 19, 2012My favourite book of all time, it's a really great read. I just finished reading 'Ocean Depths'.\n",
      "\n",
      "Reviewer: davemck - favorite favorite favorite favorite favorite - November 9, 2012\n",
      "\n",
      "Subject: Great book This is a great book. I'm a huge fan of this book and am excited to have seen it again. - November 9, 2012Great book\n",
      "\n",
      "\n",
      "------\n",
      "Parker: Thinking of getting a smartwatch. Which one do you use? Apple Watch Apple Watch with Face ID\n",
      "\n",
      "There are many more options for people to wear smartwatches and their gadgets, but Apple is making the most of its current crop of smartwatches. The company has launched two smartwatches and a new watch called the Apple Watch that can run on the Apple Watch app. It has a 3.5-inch OLED screen and a 4.4-inch screen that's thinner than most other smartwatches. It's also possible to get the Apple Watch to watch video without your watch, so you can watch the movie without having to worry about\n",
      "------\n",
      "Drew: Remember that trip to Paris? Found our goofy photo at the Louvre. All I want for Christmas is Christmas. Christmas really is a big one. I'm still trying to figure out the answer.\n",
      "\n",
      "We do miss that old family picture on the cover of my book when we went back. I hope to put it on the cover this year, but I'm looking at my book this year. Thank you to you Santa. You did the best in putting it in my book.\n",
      "\n",
      "Thank you so much. My favorite part. That first picture of you with the family is from a friend who saw your book and was able to get it for me. I want\n",
      "------\n",
      "Riley: I tried that new pasta recipe you sent. My family loved it! The meat is really nice and not too big. The sauce is a bit more dense than my normal sauce, but has a slight flavor. Overall, this dish is pretty good for everyone that wants to get their hands dirty.\n",
      "\n",
      "I have to start by mentioning that I ordered this post because I'm not a meat lover or meat lover. The flavors on this sauce and broth are very good. It was the combination of spices that really pushed my palate to the max for this dish.\n",
      "\n",
      "If you\n",
      "------\n",
      "Riley: Your photos are amazing. Got any tips for a newbie? Send us a message.\n",
      "\n",
      "Amber: I just saw you on the front page of the Daily Mail! I was so excited to see you in person. I've been waiting for the opportunity to see you for a while now, but I'm so glad you're here! I'm really looking forward to seeing you at the show!\n",
      "\n",
      "Riley: I love you so much. I love the way you dress and the way you talk. I'm so excited to see you at the show.\n",
      "\n",
      "Amber: You're so cute! I love you so much!\n",
      "\n",
      "Riley: I love you so much!\n",
      "\n",
      "Amber: Thank you for\n",
      "------\n",
      "Charlie: Movie night this Friday? I'll bring the popcorn. There are many movies and TV shows.\n",
      "\n",
      "The only thing I'd want is the popcorn. It's good.\n",
      "\n",
      "When you go to a movie and you walk in and you don't have a seat, you have to get the popcorn. And I don't have a seat.\n",
      "\n",
      "The thing is, I'm really scared of popcorn.\n",
      "\n",
      "So I've gotten a lot of requests for me to come up with something that I could share with the audience that they'll like and that they'd want to see\n",
      "------\n",
      "Riley: Your photos are amazing. Got any tips for a newbie? We'd love to hear them. Have you used any of the photos yet? How did you make the photos?\n",
      "\n",
      "Reiley: Thanks for the tip. I'll see you soon.\n",
      "\n",
      "Riley: Thanks for taking the time to answer the question!\n",
      "\n",
      "Riley: We have an update for you.\n",
      "\n",
      "I hope that answers all of your questions.\n",
      "\n",
      "------\n",
      "Sam: How's your Spanish going? Picked up any new phrases? Yielded the book as an eBook. How much time have you spent working on it? 3 Months 3 Years 1 Year - 2018 12 months 6 years - 2016 1 Year - 2018 13 months 8 years - 2015 3 Months 2 Years - 2018 13 months 11 years - 2014 5 Months 3 Years - 2017 8 Months 3\n",
      "------\n",
      "Alex: Just got tickets to Lively Beats fest! Ever been? I don't know... I know, like, every single person I meet who is out there making music for a living and is trying out their music... or anything. It's pretty amazing! Alex: Yeah, I love that! That's so cool! Alex: That's what's exciting about Lively. It's just kind of, like, it's a different kind of experience, and there's no other way to go from where I'm and be like, \"Oh, wow, there's a new album in the works. It's really cool.\" It's\n",
      "------\n",
      "Morgan: Got any exciting plans for the weekend? Have a question about a specific topic? Send us an email to info@theatlantic.com or send an e-mail to info@theatlantic.com with \"I'm interested in Morgan: Got any exciting plans for the weekend?\"\n",
      "\n",
      "What are you doing in the morning, and what's your schedule like? What do you do in the morning? Are you ready to start planning? Do you have a job on the schedule? What do you do in the\n",
      "------\n",
      "Taylor: I just finished reading 'Ocean Depths'. Blew my mind.\n",
      "\n",
      "Tessa: Wow.\n",
      "\n",
      "Taylor: Oh, I saw you were talking about this one. I mean, yeah, you're a writer.\n",
      "\n",
      "Tessa: Yeah, but I love that thing. I love that thing. It's a bit of a dark and gritty world of the ocean, and there are all kinds of mysteries and there's all kinds of\n",
      "------\n",
      "Morgan: I'm trying to drink more water. Got any tips for staying hydrated? I've had the luxury of eating less, but I'm a bit dehydrated now. What is the best thing about drinking less water in general? I think the most important thing to remember is to use a high-powered toilet. It's better to get rid of the water than the water that is going into your pores, so that's what I recommend. What are some other tips I've heard from people that you might not know? Water in general, if you have an allergy or any other health concerns, you need to get help.\n",
      "\n",
      "Drinking more water in general, if you have an allergy or any other health concerns, you need to get help.\n",
      "------\n",
      "Jamie: So, where are we off to next? Mountains or beach?\n",
      "\n",
      "Mike: I'm heading to the Pacific Northwest. We'll probably be on the North Shore, but I don't know. I'm really just going to head to the West Coast.\n",
      "\n",
      "Jamie: It's probably the same, but I'm not\n",
      "------\n",
      "Quinn: My cat did the funniest thing yesterday. We got our own video, the one that looks like this:\n",
      "\n",
      "And then I found the link to the first version, on Twitter:\n",
      "------\n",
      "Alex: Trying to live more sustainably. Where do I start?\n",
      "\n",
      "Mike: We have a sustainable sustainability plan. We have a working program, we do what we do best. We know how much water we can save from our water and what we do best, but we also know how much we need to spend to save that water. We do this because it's what we love to do in our community and because it's what we have to do.\n",
      "\n",
      "What's your favorite food or drink?\n",
      "\n",
      "Alex: It depends on what you're looking for. My favorites are the sweet potato pancakes. I love them, I like them sweet potato pancakes. It's a nice sweet potato pancake.\n",
      "\n",
      "What's your favorite pasta?\n",
      "\n",
      "Alex\n",
      "------\n",
      "Alex: Just got tickets to Lively Beats fest! Ever been? I'm here and I'm going to see you there! I'm pretty excited for you, Alex! I'm Alex!\n",
      "\n",
      "Alex: Alex, let's see if I can give you the first song you've ever written.\n",
      "\n",
      "Alex: I'm Alex. I'm Alex!\n",
      "\n",
      "Alex: (laughs) I'm Alex!\n",
      "\n",
      "Alex\n",
      "------\n",
      "Bailey: I'm thinking of switching careers. Ever done something that bold? Bailey: I did. It was a big deal, really. It was one of the hardest decisions that I've ever made. I don't have the energy to go and do it.\n",
      "\n",
      "Duke fans, on the other hand, know that the Cavaliers have won a ton of games at home in the last two years and are going to do it again this year. And the Cavaliers have shown that they can do it against a team that has no real playoff chance. It's going to be hard to do without Kyrie Irving and Dwyane Wade.\n",
      "\n",
      "\n",
      "------\n",
      "Sam: How's your Spanish going? Picked up any new phrases? What's the best way to say it? How many times? What's your favourite kind of music?\n",
      "\n",
      "Czechs: We're all Czechs!\n",
      "\n",
      "Sam: What's your favourite dish?\n",
      "\n",
      "Czechs: I'm making you a Czech pizza\n",
      "\n",
      "Sam: What's your favourite drink?\n",
      "\n",
      "Czechs: That's Czech!\n",
      "\n",
      "Sam: What's your favourite dish?\n",
      "\n",
      "Czechs: That's Czech!\n",
      "\n",
      "Sam: What's your favourite food?\n",
      "\n",
      "Czechs: I'm making you a Czech pizza\n",
      "\n",
      "Sam: What's your favourite drink?\n",
      "\n",
      "Czechs:\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "class DialogSandbox:\n",
    "    def __init__(self, model_name='gpt2', device='cpu'):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "        self.device = device\n",
    "    \n",
    "    def generate_conversations(self, num_conversations=10):\n",
    "        conversations = []\n",
    "        for _ in range(num_conversations):\n",
    "            prompt, initial_sentence = self._select_random_prompt()\n",
    "            dialog = self.generate_dialog(prompt, initial_sentence, \n",
    "                                          max_length=random.randint(50, 150), \n",
    "                                          num_return_sequences=1, \n",
    "                                          temperature=random.uniform(0.7, 1.2), \n",
    "                                          top_p=random.uniform(0.8, 0.95))\n",
    "            conversations.append(dialog[0])\n",
    "        return conversations\n",
    "\n",
    "    \n",
    "    def _select_random_prompt(self):\n",
    "        prompts = [\n",
    "            (\"After a music festival\", \"Alex: Did you see the final act last night? Unbelievable!\"),\n",
    "            (\"Planning a trip\", \"Jamie: So, where are we off to next? Mountains or beach?\"),\n",
    "            (\"Discussing a new series\", \"Sam: Have you started watching 'The Great Adventure' yet?\"),\n",
    "            (\"Morning routines\", \"Casey: I've switched to yoga in the mornings. Best decision ever.\"),\n",
    "            (\"Workout motivation\", \"Jordan: How do you get yourself to the gym every day? I need tips.\"),\n",
    "            (\"Favorite books\", \"Taylor: I just finished reading 'Ocean Depths'. Blew my mind.\"),\n",
    "            (\"Weekend plans\", \"Morgan: Got any exciting plans for the weekend?\"),\n",
    "            (\"Sharing recipes\", \"Riley: I tried that new pasta recipe you sent. My family loved it!\"),\n",
    "            (\"Pet stories\", \"Quinn: My cat did the funniest thing yesterday.\"),\n",
    "            (\"Tech gadgets\", \"Parker: Thinking of getting a smartwatch. Which one do you use?\"),\n",
    "            (\"Study tips\", \"Robin: Finals are coming up. How do you keep yourself focused?\"),\n",
    "            (\"Vacation memories\", \"Drew: Remember that trip to Paris? Found our goofy photo at the Louvre.\"),\n",
    "            (\"Movie night\", \"Charlie: Movie night this Friday? I'll bring the popcorn.\"),\n",
    "            (\"Career advice\", \"Bailey: I'm thinking of switching careers. Ever done something that bold?\"),\n",
    "            (\"Fashion trends\", \"Taylor: How do you always stay ahead with fashion trends?\"),\n",
    "            (\"Concert experiences\", \"Alex: Just got tickets to Lively Beats fest! Ever been?\"),\n",
    "            (\"Childhood memories\", \"Casey: What's your happiest childhood memory?\"),\n",
    "            (\"Dinner disaster stories\", \"Jordan: Ever had a complete kitchen fail? I just did.\"),\n",
    "            (\"Learning languages\", \"Sam: How's your Spanish going? Picked up any new phrases?\"),\n",
    "            (\"Outdoor adventures\", \"Jamie: Ever been rock climbing? Looking for a buddy to try it with.\"),\n",
    "            (\"Health and wellness\", \"Morgan: I'm trying to drink more water. Got any tips for staying hydrated?\"),\n",
    "            (\"Photography tips\", \"Riley: Your photos are amazing. Got any tips for a newbie?\"),\n",
    "            (\"Book club suggestions\", \"Quinn: Our book club needs a new read. Any genre preferences?\"),\n",
    "            (\"Weekend DIY projects\", \"Parker: Planning a DIY project this weekend. Ever built a bookshelf?\"),\n",
    "            (\"Coffee lovers\", \"Robin: Discovered a new coffee shop downtown. Fancy a cup this weekend?\"),\n",
    "            (\"Music recommendations\", \"Drew: In a music rut. Send me your top three songs?\"),\n",
    "            (\"Fitness challenges\", \"Charlie: Signed up for a 10k run. Ever done one?\"),\n",
    "            (\"Budget travel tips\", \"Bailey: Planning a budget trip to Asia. Any advice?\"),\n",
    "            (\"Cultural exchange\", \"Taylor: What's a tradition from your culture I should know about?\"),\n",
    "            (\"Sustainability practices\", \"Alex: Trying to live more sustainably. Where do I start?\")\n",
    "        ]\n",
    "\n",
    "        \n",
    "        return random.choice(prompts)\n",
    "\n",
    "\n",
    "    def generate_dialog(self, prompt, initial_sentence, max_length=100, num_return_sequences=1, temperature=1.0, top_p=0.9):\n",
    "        \"\"\"\n",
    "        Generate dialog based on a given prompt using GPT-2, excluding the initial prompt from the output.\n",
    "        \n",
    "        :param prompt: Complete prompt for the dialog generation, including instructions.\n",
    "        :param initial_sentence: The initial sentence of the dialogue to include in the output.\n",
    "        :param max_length: Maximum length of the generated dialog.\n",
    "        :param num_return_sequences: Number of dialog sequences to generate.\n",
    "        :param temperature: Sampling temperature for diversity.\n",
    "        :param top_p: Nucleus sampling parameter for controlling diversity.\n",
    "        :return: Generated dialog sequences, starting with the initial sentence.\n",
    "        \"\"\"\n",
    "        full_prompt = \" \"+ prompt+ \" \" +initial_sentence\n",
    "\n",
    "        # Prepare the prompt\n",
    "        input_ids = self.tokenizer.encode(full_prompt, return_tensors='pt').to(self.device)\n",
    "\n",
    "        # Generate sequences\n",
    "        chat_outputs = self.model.generate(\n",
    "            input_ids,\n",
    "            max_length=max_length + len(input_ids[0]),\n",
    "            num_return_sequences=num_return_sequences,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            pad_token_id=self.tokenizer.eos_token_id,\n",
    "            do_sample=True  # Ensure sampling is enabled\n",
    "        )\n",
    "        \n",
    "        # Decode the generated sequences and trim the instruction part\n",
    "        generated_dialogs = [self.tokenizer.decode(output, skip_special_tokens=True) for output in chat_outputs]\n",
    "        trimmed_dialogs = [dialog.replace(full_prompt, initial_sentence, 1) for dialog in generated_dialogs]\n",
    "\n",
    "        return trimmed_dialogs\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    sandbox = DialogSandbox(model_name='gpt2', device='cpu')\n",
    "    conversations = sandbox.generate_conversations(num_conversations=20)\n",
    "    for dialog in conversations:\n",
    "        print(dialog)\n",
    "        print(\"------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full_prompt=' A conversation between a novice cook and a more experienced friend about cooking tips: Taylor: I always end up burning the garlic. Any tips?'\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import json\n",
    "import os\n",
    "\n",
    "class DialogSandbox:\n",
    "    def __init__(self, model_name='gpt2', device='cpu'):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "        self.device = device\n",
    "    \n",
    "    def generate_conversations(self, num_conversations=10):\n",
    "        conversations = []\n",
    "        for _ in range(num_conversations):\n",
    "            prompt, initial_sentence = self._select_random_prompt()\n",
    "            dialog = self.generate_dialog(prompt, initial_sentence, \n",
    "                                          max_length=random.randint(50, 150), \n",
    "                                          num_return_sequences=1, \n",
    "                                          temperature=random.uniform(0.7, 1.2), \n",
    "                                          top_p=random.uniform(0.8, 0.95))\n",
    "            conversations.append(dialog[0])\n",
    "        print(conversations)\n",
    "        return conversations\n",
    "\n",
    "    \n",
    "    def _select_random_prompt(self):\n",
    "        # /users/k/n/kngongiv/Research/private_llm_generation/dialogue/stage1/summary/data/prefix/dialog_prefix.json\n",
    "        project_dir = os.path.dirname(\"./stage1/summary/\")\n",
    "        with open(os.path.join(project_dir, f\"data/prefix/dialog_prefix.json\"), \"r\") as f:\n",
    "            prefix_resource = json.load(f)\n",
    "    \n",
    "        scenario = random.choice(prefix_resource[\"dialog_prefixes\"])\n",
    "         #    scenario = random.choice(self.prefix_resource[\"dialog_prefixes\"])\n",
    "        prefix = scenario[\"prefix\"]\n",
    "        initial_sentence = scenario[\"initial_sentence\"]\n",
    "        return prefix, initial_sentence\n",
    "\n",
    "\n",
    "    def generate_dialog(self, prompt, initial_sentence, max_length=100, num_return_sequences=1, temperature=1.0, top_p=0.9):\n",
    "        \"\"\"\n",
    "        Generate dialog based on a given prompt using GPT-2, excluding the initial prompt from the output.\n",
    "        \n",
    "        :param prompt: Complete prompt for the dialog generation, including instructions.\n",
    "        :param initial_sentence: The initial sentence of the dialogue to include in the output.\n",
    "        :param max_length: Maximum length of the generated dialog.\n",
    "        :param num_return_sequences: Number of dialog sequences to generate.\n",
    "        :param temperature: Sampling temperature for diversity.\n",
    "        :param top_p: Nucleus sampling parameter for controlling diversity.\n",
    "        :return: Generated dialog sequences, starting with the initial sentence.\n",
    "        \"\"\"\n",
    "        full_prompt = \" \"+ prompt+ \" \" +initial_sentence\n",
    "        print(f\"{full_prompt=}\")\n",
    "        # Prepare the prompt\n",
    "        input_ids = self.tokenizer.encode(full_prompt, return_tensors='pt').to(self.device)\n",
    "\n",
    "        # Generate sequences\n",
    "        chat_outputs = self.model.generate(\n",
    "            input_ids,\n",
    "            max_length=max_length + len(input_ids[0]),\n",
    "            num_return_sequences=num_return_sequences,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            pad_token_id=self.tokenizer.eos_token_id,\n",
    "            do_sample=True  # Ensure sampling is enabled\n",
    "        )\n",
    "        \n",
    "        # Decode the generated sequences and trim the instruction part\n",
    "        generated_dialogs = [self.tokenizer.decode(output, skip_special_tokens=True) for output in chat_outputs]\n",
    "        trimmed_dialogs = [dialog.replace(full_prompt, initial_sentence, 1) for dialog in generated_dialogs]\n",
    "\n",
    "        return trimmed_dialogs\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    sandbox = DialogSandbox(model_name='gpt2', device='cpu')\n",
    "    conversations = sandbox.generate_conversations(num_conversations=3)\n",
    "    for dialog in conversations:\n",
    "        print(dialog)\n",
    "        print(\"------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# import torch\n",
    "\n",
    "# class NonInteractiveDialogSandbox:\n",
    "#     def __init__(self, model_name='microsoft/DialoGPT-large', device='cpu'):\n",
    "#         self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#         self.model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "#         self.device = device\n",
    "\n",
    "#     def generate_dialog(self, initial_prompts, num_exchange=5, max_length=1000):\n",
    "#         \"\"\"\n",
    "#         Simulate a dialogue based on initial prompts.\n",
    "        \n",
    "#         :param initial_prompts: A list of strings representing the initial exchanges.\n",
    "#         :param num_exchange: Number of exchanges to simulate.\n",
    "#         :param max_length: Maximum length of the dialogue.\n",
    "#         \"\"\"\n",
    "#         chat_history_ids = None\n",
    "#         for step, prompt in enumerate(initial_prompts + [\"\"] * (num_exchange - len(initial_prompts))):\n",
    "#             # Encode the current exchange, add the eos_token and return a tensor in Pytorch\n",
    "#             new_user_input_ids = self.tokenizer.encode(prompt + self.tokenizer.eos_token, return_tensors='pt').to(self.device)\n",
    "            \n",
    "#             # Append the new user input tokens to the chat history\n",
    "#             bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "\n",
    "#             # Generate a response\n",
    "#             chat_history_ids = self.model.generate(bot_input_ids, max_length=max_length, pad_token_id=self.tokenizer.eos_token_id)\n",
    "\n",
    "#         # Decode and print the dialogue\n",
    "#         dialogue = self.tokenizer.decode(chat_history_ids[0], skip_special_tokens=True)\n",
    "#         return dialogue\n",
    "\n",
    "# # Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "#     sandbox = NonInteractiveDialogSandbox(model_name='microsoft/DialoGPT-large', device='cpu')  # Use 'cuda' if GPU is available\n",
    "    \n",
    "#     initial_prompts = [\n",
    "#         \">> User: How's the weather today?\",\n",
    "#         \">> DialoGPT: It's sunny and clear, a beautiful day!\"\n",
    "#     ]\n",
    "    \n",
    "#     dialogue = sandbox.generate_dialog(initial_prompts, num_exchange=5)\n",
    "#     print(\"Generated Dialogue:\\n\", dialogue)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     sandbox = DialogSandbox(model_name='gpt2', device='cpu')  # Use 'cuda' if GPU is available\n",
    "    \n",
    "#     # Full prompt used for generation\n",
    "#     prompt = \"The following is a conversation between two old friends, John and Sarah, who unexpectedly meet at a park: John: Hey Sarah! It's been a long time. How have you been?\"\n",
    "\n",
    "#     # Initial sentence you want to appear at the start of each generated dialogue\n",
    "#     initial_sentence = \"John: Hey Sarah! It's been a long time. How have you been?\"\n",
    "\n",
    "#     prompt=\"Alice, a software engineer, runs into her college friend Bob, now a teacher, at a tech conference. They start catching up on their careers and share insights from their professional paths:\"\n",
    "#     # Now including the initial_sentence argument in the method call\n",
    "#     dialogs = sandbox.generate_dialog(prompt, initial_sentence, max_length=50, num_return_sequences=3, temperature=0.8, top_p=0.95)\n",
    "    \n",
    "#     for i, dialog in enumerate(dialogs, 1):\n",
    "#         print(f\"Dialog {i}:\\n{dialog}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Character Context and Background\n",
    "Prompt: \"Alice, a software engineer, runs into her college friend Bob, now a teacher, at a tech conference. They start catching up on their careers and share insights from their professional paths.\"\n",
    "2. Open-ended and Engaging Starters\n",
    "Prompt: \"During a long car ride, Charlie asks Diana about her most unforgettable travel experience, sparking a conversation about adventure, culture, and the lessons learned from traveling.\"\n",
    "3. Alternating Turns\n",
    "Prompt: \"Ethan and Fiona debate the best superhero movie of the decade. Each presents their case, citing specific scenes, character development, and overall impact, trying to sway the other's opinion.\"\n",
    "4. Emotional and Situational Variability\n",
    "Prompt: \"George is feeling down about a missed job opportunity, and Hannah offers comfort. Their conversation explores feelings of disappointment, strategies for coping, and optimism for the future.\"\n",
    "5. Adaptation and Follow-up\n",
    "Prompt: \"Ian, passionate about environmental conservation, talks to Jenny, who is curious but skeptical. Ian provides compelling arguments and facts, responding to Jenny's doubts with patience and enthusiasm.\"\n",
    "6. Closure or Continuation\n",
    "Prompt: \"After a heated discussion about a book they both read, Kevin suggests to Laura that they start a monthly book club. They brainstorm potential books, members, and meeting logistics, ending the conversation with plans to draft a list of invitees.\"\n",
    "Generating the Dialogs\n",
    "To generate dialogs based on these prompts, you could use the following structured approach for each prompt:\n",
    "\n",
    "Initialize the Conversation: Start with the given scenario, explicitly stating the setting and the characters involved.\n",
    "\n",
    "Guide the Dialog: Use the prompt to guide the initial exchanges, ensuring the conversation stays on topic and follows the intended emotional and situational direction.\n",
    "\n",
    "Diversify Responses: Encourage variability in responses to include reflections, counterpoints, and new questions, which naturally extend the dialog.\n",
    "\n",
    "Ensure Coherence: Maintain the flow of conversation, ensuring that each response is coherent with the previous lines, and reflects the progression of the dialog."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_envs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
