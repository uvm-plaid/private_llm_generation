{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import T5ForConditionalGeneration, T5Tokenizer, get_linear_schedule_with_warmup\n",
    "# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "# from datasets import load_dataset\n",
    "# from torch.utils.data import DataLoader\n",
    "# from private_transformers import PrivacyEngine\n",
    "# import torch.nn.functional as F\n",
    "# # from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "# # from transformers import T5ForConditionalGeneration, T5Tokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "# from torch.optim import AdamW  \n",
    "# import numpy as np\n",
    "# import os\n",
    "# import wandb\n",
    "# from datasets import Dataset\n",
    "# import pandas as pd\n",
    "\n",
    "# import spacy\n",
    "# spacy_model = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "# def preprocess_function(batch, tokenizer, max_input_length=512, max_output_length=80):\n",
    "#     inputs = tokenizer(batch[\"dialogue\"], max_length=max_input_length, padding=\"max_length\", truncation=True, return_tensors='pt')\n",
    "#     targets = tokenizer(batch[\"summary\"], max_length=max_output_length, padding=\"max_length\", truncation=True, return_tensors='pt')\n",
    "    \n",
    "#     print(\"Inputs: \",inputs )\n",
    "#     print(\"Inputs: \",targets )\n",
    "#     # Preparing the dataset to be returned\n",
    "#     model_inputs = {key: inputs[key] for key in inputs}\n",
    "#     model_inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    \n",
    "#     return model_inputs\n",
    "\n",
    "\n",
    "# def find_latest_checkpoint(checkpoint_dir):\n",
    "#     checkpoint_subdirs = [d for d in os.listdir(checkpoint_dir) if os.path.isdir(os.path.join(checkpoint_dir, d))]\n",
    "#     if not checkpoint_subdirs:\n",
    "#         return None\n",
    "#     latest_checkpoint = max(checkpoint_subdirs, key=lambda d: int(d.split('-')[-1]))\n",
    "#     return os.path.join(checkpoint_dir, latest_checkpoint)\n",
    "\n",
    "# def compute_average_length(dataset):\n",
    "#     lengths = dataset[\"lengths\"]  # Assuming 'lengths' are stored in the dataset\n",
    "#     average_length = np.mean(lengths)\n",
    "#     print(f\"Average token length: {average_length}\")\n",
    "\n",
    "# wandb.init(project=\"Summarize_DP_Dialog\", entity=\"ivolinengong\", name=\"dp\")\n",
    "\n",
    "# # Check if CUDA is available and set the device\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # Set seeds for reproducibility\n",
    "# torch.manual_seed(42)\n",
    "\n",
    "# # Load the tokenizer\n",
    "# max_input_length = 200 #1024\n",
    "# max_output_length = 100 #256 #250\n",
    "# batch_size = 5\n",
    "# epochs = 8\n",
    "# gradient_accumulation_steps = 10\n",
    "\n",
    "# # Load SAMSum dataset\n",
    "# train_dataset = load_dataset(\"samsum\", split=\"train\")\n",
    "# val_dataset = load_dataset(\"samsum\", split=\"validation\")\n",
    "\n",
    "# # Checkpoint directories\n",
    "# checkpoint_dir = f\"./dp/dp_results/{wandb.run.name}\"\n",
    "# # checkpoint_dir = f\"./results/dp_results/test/rich-serenity-9\"\n",
    "# os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# model_tokenizer_path = checkpoint_dir\n",
    "# optimizer_checkpoint_path = os.path.join(checkpoint_dir, \"optimizer_and_loss.pth\")\n",
    "\n",
    "# # Load model and tokenizer, and optimizer state if available\n",
    "# model_checkpoint = find_latest_checkpoint(checkpoint_dir)\n",
    "# if model_checkpoint:\n",
    "#     print(f\"Resuming from checkpoint: {model_checkpoint}\")\n",
    "\n",
    "#     # Load model and tokenizer from the checkpoint\n",
    "#     model = T5ForConditionalGeneration.from_pretrained(model_checkpoint).to(device)\n",
    "#     tokenizer = T5Tokenizer.from_pretrained(model_checkpoint, model_max_length=max_input_length)\n",
    "\n",
    "#     # Load optimizer state\n",
    "#     if os.path.isfile(optimizer_checkpoint_path):\n",
    "#         print(\"Loading optimizer state and best loss from checkpoint\")\n",
    "#         checkpoint = torch.load(optimizer_checkpoint_path, map_location=device)\n",
    "#         optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "#         optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "#         best_loss = checkpoint['best_loss']\n",
    "# else:\n",
    "#     print(\"Starting training from the pretrained model\")\n",
    "#     model = T5ForConditionalGeneration.from_pretrained(\"t5-large\").to(device)\n",
    "#     tokenizer = T5Tokenizer.from_pretrained(\"t5-large\", model_max_length=max_input_length)\n",
    "#     optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "#     best_loss = float('inf')\n",
    "\n",
    "# # Initialize tokenizer and model\n",
    "# tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "# model = T5ForConditionalGeneration.from_pretrained(\"t5-base\").to(device)\n",
    "\n",
    "# # Tokenize and preprocess datasets\n",
    "# tokenized_train_dataset = train_dataset.map(lambda batch: preprocess_function(batch, tokenizer, max_input_length, max_output_length), batched=True)\n",
    "# # tokenized_val_dataset = val_dataset.map(lambda batch: preprocess_function(batch, tokenizer, max_input_length, max_output_length), batched=True)\n",
    "\n",
    "# print(\"\\nTraining dataset size:\", len(tokenized_train_dataset))\n",
    "\n",
    "# # # Inspect a sample\n",
    "# sample = tokenized_train_dataset[0]\n",
    "# # print(f\"Sample keys: {sample.keys()}\")\n",
    "# # print(f\"Input IDs shape: {sample['input_ids'].shape}\")\n",
    "# # print(f\"Attention mask shape: {sample['attention_mask'].shape}\")\n",
    "# # print(f\"Labels shape: {sample['labels'].shape}\")\n",
    "\n",
    "# print(f\"Sample keys: {sample.keys()}\")\n",
    "# print(f\"Input IDs shape: {torch.tensor(sample['input_ids']).shape}\")\n",
    "# print(f\"Attention mask shape: {torch.tensor(sample['attention_mask']).shape}\")\n",
    "# print(f\"Labels shape: {torch.tensor(sample['labels']).shape}\")\n",
    "\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # Attach the privacy engine to the optimizer\n",
    "# privacy_engine = PrivacyEngine(\n",
    "#     model,\n",
    "#     batch_size=batch_size,\n",
    "#     sample_size=len(tokenized_train_dataset),\n",
    "#     epochs=epochs,\n",
    "#     max_grad_norm=1.0,\n",
    "#     target_epsilon=8,\n",
    "#     noise_multiplier=0.1,\n",
    "#     # clipping_mode=\"ghost\",\n",
    "# )\n",
    "# privacy_engine.attach(optimizer)\n",
    "\n",
    "# # Learning rate scheduler\n",
    "# total_steps = len(tokenized_train_dataset) // batch_size * epochs\n",
    "# scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "# data_loader = DataLoader(tokenized_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# # Training loop\n",
    "# model.train()\n",
    "# for epoch in range(epochs):\n",
    "#     total_loss = 0\n",
    "#     print(f\"\\nStarting epoch {epoch + 1}/{epochs}\", flush=True)\n",
    "#     for i, batch in enumerate(data_loader):\n",
    "#         # input_ids = batch['input_ids'].to(device)\n",
    "#         # attention_mask = batch['attention_mask'].to(device)\n",
    "#         # labels = batch['labels'].to(device)\n",
    "\n",
    "#         input_ids = batch['input_ids'].to(device)\n",
    "#         attention_mask = batch['attention_mask'].to(device)\n",
    "#         labels = batch['labels'].to(device)\n",
    "\n",
    "#         # input_ids = torch.tensor(batch['input_ids']).to(device)\n",
    "#         # attention_mask = torch.tensor(batch['attention_mask']).to(device)\n",
    "#         # labels = torch.tensor(batch['labels']).to(device)\n",
    "\n",
    "#         # Forward pass\n",
    "#         outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "#         logits = outputs.logits\n",
    "\n",
    "#         # Compute per-example loss\n",
    "#         shift_logits = logits[..., :-1, :].contiguous()\n",
    "#         shift_labels = labels[..., 1:].contiguous()\n",
    "#         loss = F.cross_entropy(shift_logits.permute(0, 2, 1), shift_labels, reduction=\"none\").mean(dim=1)\n",
    "\n",
    "#         # # Optimization step (without calling loss.backward())\n",
    "#         # optimizer.step(loss=loss)\n",
    "\n",
    "#         # optimizer.zero_grad()\n",
    "#         # scheduler.step()\n",
    "        \n",
    "#         # Gradient accumulation\n",
    "#         if (i + 1) % gradient_accumulation_steps == 0:\n",
    "#             optimizer.step(loss=loss)  # Perform parameter update\n",
    "#             optimizer.zero_grad()  # Reset gradients\n",
    "#             scheduler.step()\n",
    "#         else:\n",
    "#             optimizer.virtual_step(loss=loss)  # Accumulate gradients\n",
    "\n",
    "\n",
    "#         # Save the model if this is the best loss so far\n",
    "#         avg_loss = torch.mean(loss).item()\n",
    "#         total_loss += avg_loss\n",
    "#         # Log metrics to wandb\n",
    "#         # wandb.log({\"Epoch\": epoch + 1, \"Step\": i, \"Loss\": avg_loss})\n",
    "        \n",
    "#         # print(f\"Epoch: {epoch + 1}, Step: {i}/{len(data_loader)}, Loss: {avg_loss}\\n\")\n",
    "\n",
    "#         # Check if the current loss is the best\n",
    "#         if avg_loss < best_loss:\n",
    "#             best_loss = avg_loss\n",
    "\n",
    "#             # Save model and tokenizer using save_pretrained\n",
    "#             model.save_pretrained(checkpoint_dir)\n",
    "#             tokenizer.save_pretrained(checkpoint_dir)\n",
    "\n",
    "#             # Save optimizer state and best_loss separately\n",
    "#             torch.save({\n",
    "#                 'optimizer_state_dict': optimizer.state_dict(),\n",
    "#                 'best_loss': best_loss,\n",
    "#             }, optimizer_checkpoint_path)\n",
    "\n",
    "#             print(f\"New best model saved with loss: {best_loss}\")\n",
    "\n",
    "#         # # Log progress\n",
    "#         if i % 10 == 0:\n",
    "#             print(f\"Epoch: {epoch + 1}, Step: {i}/{len(data_loader)}, Loss: {avg_loss}\", flush=True)\n",
    "\n",
    "#     # Calculate the average training loss and perplexity for the epoch\n",
    "#     avg_train_loss = total_loss / len(data_loader)\n",
    "#     train_perplexity = np.exp(avg_train_loss)\n",
    "#     wandb.log({\"epoch\": epoch + 1, \"train_loss\": avg_train_loss, \"train_perplexity\": train_perplexity})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mivolinengong\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/gpfs1/home/k/n/kngongiv/Research/impossible_kd/dialogue/wandb/run-20240220_105559-nrnzfc0n</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ivolinengong/Summarize_DP_PubMed/runs/nrnzfc0n' target=\"_blank\">flashing-wonton-9</a></strong> to <a href='https://wandb.ai/ivolinengong/Summarize_DP_PubMed' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ivolinengong/Summarize_DP_PubMed' target=\"_blank\">https://wandb.ai/ivolinengong/Summarize_DP_PubMed</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ivolinengong/Summarize_DP_PubMed/runs/nrnzfc0n' target=\"_blank\">https://wandb.ai/ivolinengong/Summarize_DP_PubMed/runs/nrnzfc0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training from the pretrained model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39195bbd722a4f7aa30463bd7b1b6482",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14732 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/k/n/kngongiv/miniconda3/envs/pytorch_envs/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3860: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5cfe3830ae3435b8aaaa9169b0b4dca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/818 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training dataset size: 14732\n",
      "Validation dataset size: 818\n",
      "Sample keys: dict_keys(['input_ids', 'attention_mask', 'labels'])\n",
      "Input IDs shape: torch.Size([150])\n",
      "Attention mask shape: torch.Size([150])\n",
      "Labels shape: torch.Size([100])\n",
      "\n",
      "Starting epoch 1/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/k/n/kngongiv/miniconda3/envs/pytorch_envs/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, get_linear_schedule_with_warmup\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from private_transformers import PrivacyEngine\n",
    "import torch.nn.functional as F\n",
    "# from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "# from transformers import T5ForConditionalGeneration, T5Tokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW  \n",
    "import numpy as np\n",
    "import os\n",
    "import wandb\n",
    "import spacy\n",
    "\n",
    "# Load the SpaCy model for sentence tokenization\n",
    "spacy_model = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "# def preprocess_function(examples):\n",
    "#     # Use spacy to extract first 3 sentences as a simplification\n",
    "#     def get_first_3_sentences(text):\n",
    "#         doc = spacy_model(text)\n",
    "#         sentences = [sent.text.strip() for sent in doc.sents][:3]\n",
    "#         return \" \".join(sentences)\n",
    "    \n",
    "#     # Modify here to use 'dialogue' field from SAMSum dataset\n",
    "#     inputs = [\"summarize: \" + get_first_3_sentences(doc) for doc in examples[\"dialogue\"]]\n",
    "#     model_inputs = tokenizer(inputs, max_length=max_input_length, padding=\"max_length\", truncation=True)\n",
    "\n",
    "#     # 'summary' field is used as labels in SAMSum\n",
    "#     with tokenizer.as_target_tokenizer():\n",
    "#         labels = tokenizer(examples[\"summary\"], max_length=max_output_length, padding=\"max_length\", truncation=True)\n",
    "\n",
    "#     model_inputs[\"input_ids\"] = torch.tensor(model_inputs[\"input_ids\"])\n",
    "#     model_inputs[\"attention_mask\"] = torch.tensor(model_inputs[\"attention_mask\"])\n",
    "#     model_inputs[\"labels\"] = torch.tensor(labels[\"input_ids\"])\n",
    "#     return model_inputs\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [\"summarize: \" + doc for doc in examples[\"dialogue\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, padding=\"max_length\", truncation=True)\n",
    "\n",
    "    # 'summary' field is used as labels in SAMSum\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"summary\"], max_length=max_output_length, padding=\"max_length\", truncation=True)\n",
    "\n",
    "    model_inputs[\"input_ids\"] = torch.tensor(model_inputs[\"input_ids\"])\n",
    "    model_inputs[\"attention_mask\"] = torch.tensor(model_inputs[\"attention_mask\"])\n",
    "    model_inputs[\"labels\"] = torch.tensor(labels[\"input_ids\"])\n",
    "    return model_inputs\n",
    "\n",
    "def find_latest_checkpoint(checkpoint_dir):\n",
    "    checkpoint_subdirs = [d for d in os.listdir(checkpoint_dir) if os.path.isdir(os.path.join(checkpoint_dir, d))]\n",
    "    if not checkpoint_subdirs:\n",
    "        return None\n",
    "    latest_checkpoint = max(checkpoint_subdirs, key=lambda d: int(d.split('-')[-1]))\n",
    "    return os.path.join(checkpoint_dir, latest_checkpoint)\n",
    "\n",
    "def compute_average_length(dataset):\n",
    "    lengths = dataset[\"lengths\"]  # Assuming 'lengths' are stored in the dataset\n",
    "    average_length = np.mean(lengths)\n",
    "    print(f\"Average token length: {average_length}\")\n",
    "\n",
    "wandb.init(project=\"Summarize_DP_PubMed\", entity=\"ivolinengong\")\n",
    "\n",
    "\n",
    "# Check if CUDA is available and set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Load the tokenizer\n",
    "max_input_length = 150 #1024\n",
    "max_output_length = 100 #256 #250\n",
    "batch_size = 5\n",
    "epochs = 8\n",
    "gradient_accumulation_steps = 10\n",
    "\n",
    "# Load pubmed dataset\n",
    "# Load SAMSum dataset\n",
    "train_dataset = load_dataset(\"samsum\", split=\"train\")\n",
    "val_dataset = load_dataset(\"samsum\", split=\"validation\")\n",
    "\n",
    "# tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
    "# train_dataset = tokenized_datasets[\"train\")\n",
    "\n",
    "# Checkpoint directories\n",
    "# checkpoint_dir = \"./results/dp_results/test/run\"\n",
    "checkpoint_dir = f\"./results/dp_results/{wandb.run.name}\"\n",
    "# checkpoint_dir = f\"./results/dp_results/test/rich-serenity-9\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "model_tokenizer_path = checkpoint_dir\n",
    "optimizer_checkpoint_path = os.path.join(checkpoint_dir, \"optimizer_and_loss.pth\")\n",
    "\n",
    "# Load model and tokenizer, and optimizer state if available\n",
    "# model_checkpoint = find_latest_checkpoint(checkpoint_dir)\n",
    "# if model_checkpoint:\n",
    "#     print(f\"Resuming from checkpoint: {model_checkpoint}\")\n",
    "#     # model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint).to(device)\n",
    "#     # tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "#     model = T5ForConditionalGeneration.from_pretrained(\"t5-large\").to(device)\n",
    "#     tokenizer = T5Tokenizer.from_pretrained(\"t5-large\", model_max_length=max_input_length)\n",
    "\n",
    "#     if os.path.isfile(optimizer_checkpoint_path):\n",
    "#         print(\"Loading optimizer state and best loss from checkpoint\")\n",
    "#         checkpoint = torch.load(optimizer_checkpoint_path, map_location=device)\n",
    "#         optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "#         optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "#         best_loss = checkpoint['best_loss']\n",
    "model_checkpoint = find_latest_checkpoint(checkpoint_dir)\n",
    "if model_checkpoint:\n",
    "    print(f\"Resuming from checkpoint: {model_checkpoint}\")\n",
    "\n",
    "    # Load model and tokenizer from the checkpoint\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_checkpoint).to(device)\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_checkpoint, model_max_length=max_input_length)\n",
    "\n",
    "    # Load optimizer state\n",
    "    if os.path.isfile(optimizer_checkpoint_path):\n",
    "        print(\"Loading optimizer state and best loss from checkpoint\")\n",
    "        checkpoint = torch.load(optimizer_checkpoint_path, map_location=device)\n",
    "        optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        best_loss = checkpoint['best_loss']\n",
    "else:\n",
    "    print(\"Starting training from the pretrained model\")\n",
    "    model = T5ForConditionalGeneration.from_pretrained(\"t5-large\").to(device)\n",
    "    tokenizer = T5Tokenizer.from_pretrained(\"t5-large\", model_max_length=max_input_length)\n",
    "\n",
    "    # model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-large\").to(device)\n",
    "    # tokenizer = AutoTokenizer.from_pretrained(\"t5-large\", model_max_length=max_input_length)\n",
    "    optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "    best_loss = float('inf')\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_val_dataset = val_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "tokenized_val_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "\n",
    "print(\"\\nTraining dataset size:\", len(tokenized_train_dataset))\n",
    "print(\"Validation dataset size:\", len(tokenized_val_dataset))\n",
    "\n",
    "\n",
    "# # Inspect a sample\n",
    "sample = tokenized_train_dataset[0]\n",
    "print(f\"Sample keys: {sample.keys()}\")\n",
    "print(f\"Input IDs shape: {sample['input_ids'].shape}\")\n",
    "print(f\"Attention mask shape: {sample['attention_mask'].shape}\")\n",
    "print(f\"Labels shape: {sample['labels'].shape}\")\n",
    "# Calculate and print average token length\n",
    "# compute_average_length(tokenized_train_dataset)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Attach the privacy engine to the optimizer\n",
    "privacy_engine = PrivacyEngine(\n",
    "    model,\n",
    "    batch_size=batch_size,\n",
    "    sample_size=len(tokenized_train_dataset),\n",
    "    epochs=epochs,\n",
    "    max_grad_norm=1.0,\n",
    "    target_epsilon=3,\n",
    "    noise_multiplier=0.1,\n",
    "    # clipping_mode=\"ghost\",\n",
    ")\n",
    "privacy_engine.attach(optimizer)\n",
    "\n",
    "# Learning rate scheduler\n",
    "total_steps = len(tokenized_train_dataset) // batch_size * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "# Data loader\n",
    "data_loader = DataLoader(tokenized_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_data_loader = DataLoader(tokenized_val_dataset, batch_size=batch_size)\n",
    "\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    print(f\"\\nStarting epoch {epoch + 1}/{epochs}\", flush=True)\n",
    "    for i, batch in enumerate(data_loader):\n",
    "        # Move batch to device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Compute per-example loss\n",
    "        shift_logits = logits[..., :-1, :].contiguous()\n",
    "        shift_labels = labels[..., 1:].contiguous()\n",
    "        loss = F.cross_entropy(shift_logits.permute(0, 2, 1), shift_labels, reduction=\"none\").mean(dim=1)\n",
    "\n",
    "        # Gradient accumulation\n",
    "        if (i + 1) % gradient_accumulation_steps == 0:\n",
    "            optimizer.step(loss=loss)  # Perform parameter update\n",
    "            optimizer.zero_grad()  # Reset gradients\n",
    "            scheduler.step()\n",
    "        else:\n",
    "            optimizer.virtual_step(loss=loss)  # Accumulate gradients\n",
    "\n",
    "\n",
    "        # Save the model if this is the best loss so far\n",
    "        avg_loss = torch.mean(loss).item()\n",
    "        total_loss += avg_loss\n",
    "\n",
    "        # Check if the current loss is the best\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "\n",
    "            # Save model and tokenizer using save_pretrained\n",
    "            model.save_pretrained(checkpoint_dir)\n",
    "            tokenizer.save_pretrained(checkpoint_dir)\n",
    "\n",
    "            # Save optimizer state and best_loss separately\n",
    "            torch.save({\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_loss': best_loss,\n",
    "            }, optimizer_checkpoint_path)\n",
    "\n",
    "            print(f\"New best model saved with loss: {best_loss}\")\n",
    "\n",
    "        # # Log progress\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Epoch: {epoch + 1}, Step: {i}/{len(data_loader)}, Loss: {avg_loss}\", flush=True)\n",
    "\n",
    "    # Calculate the average training loss and perplexity for the epoch\n",
    "    avg_train_loss = total_loss / len(data_loader)\n",
    "    train_perplexity = np.exp(avg_train_loss)\n",
    "    wandb.log({\"epoch\": epoch + 1, \"train_loss\": avg_train_loss, \"train_perplexity\": train_perplexity})\n",
    "\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import T5ForConditionalGeneration, T5Tokenizer, get_linear_schedule_with_warmup\n",
    "# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "# from datasets import load_dataset\n",
    "# from torch.utils.data import DataLoader\n",
    "# from private_transformers import PrivacyEngine\n",
    "# import torch.nn.functional as F\n",
    "# # from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "# # from transformers import T5ForConditionalGeneration, T5Tokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "# from torch.optim import AdamW  \n",
    "# import numpy as np\n",
    "# import os\n",
    "# import wandb\n",
    "# import spacy\n",
    "\n",
    "# # Load the SpaCy model for sentence tokenization\n",
    "# spacy_model = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "# # def preprocess_function(examples):\n",
    "# #     # Use spacy to extract first 3 sentences as a simplification\n",
    "# #     def get_first_3_sentences(text):\n",
    "# #         doc = spacy_model(text)\n",
    "# #         sentences = [sent.text.strip() for sent in doc.sents][:3]\n",
    "# #         return \" \".join(sentences)\n",
    "    \n",
    "# #     # Modify here to use 'dialogue' field from SAMSum dataset\n",
    "# #     inputs = [\"summarize: \" + get_first_3_sentences(doc) for doc in examples[\"dialogue\"]]\n",
    "# #     model_inputs = tokenizer(inputs, max_length=max_input_length, padding=\"max_length\", truncation=True)\n",
    "\n",
    "# #     # 'summary' field is used as labels in SAMSum\n",
    "# #     with tokenizer.as_target_tokenizer():\n",
    "# #         labels = tokenizer(examples[\"summary\"], max_length=max_output_length, padding=\"max_length\", truncation=True)\n",
    "\n",
    "# #     model_inputs[\"input_ids\"] = torch.tensor(model_inputs[\"input_ids\"])\n",
    "# #     model_inputs[\"attention_mask\"] = torch.tensor(model_inputs[\"attention_mask\"])\n",
    "# #     model_inputs[\"labels\"] = torch.tensor(labels[\"input_ids\"])\n",
    "# #     return model_inputs\n",
    "\n",
    "# def preprocess_function(examples):\n",
    "#     inputs = [\"summarize: \" + doc for doc in examples[\"dialogue\"]]\n",
    "#     model_inputs = tokenizer(inputs, max_length=max_input_length, padding=\"max_length\", truncation=True)\n",
    "\n",
    "#     # 'summary' field is used as labels in SAMSum\n",
    "#     with tokenizer.as_target_tokenizer():\n",
    "#         labels = tokenizer(examples[\"summary\"], max_length=max_output_length, padding=\"max_length\", truncation=True)\n",
    "\n",
    "#     model_inputs[\"input_ids\"] = torch.tensor(model_inputs[\"input_ids\"])\n",
    "#     model_inputs[\"attention_mask\"] = torch.tensor(model_inputs[\"attention_mask\"])\n",
    "#     model_inputs[\"labels\"] = torch.tensor(labels[\"input_ids\"])\n",
    "#     return model_inputs\n",
    "\n",
    "# def find_latest_checkpoint(checkpoint_dir):\n",
    "#     checkpoint_subdirs = [d for d in os.listdir(checkpoint_dir) if os.path.isdir(os.path.join(checkpoint_dir, d))]\n",
    "#     if not checkpoint_subdirs:\n",
    "#         return None\n",
    "#     latest_checkpoint = max(checkpoint_subdirs, key=lambda d: int(d.split('-')[-1]))\n",
    "#     return os.path.join(checkpoint_dir, latest_checkpoint)\n",
    "\n",
    "# def compute_average_length(dataset):\n",
    "#     lengths = dataset[\"lengths\"]  # Assuming 'lengths' are stored in the dataset\n",
    "#     average_length = np.mean(lengths)\n",
    "#     print(f\"Average token length: {average_length}\")\n",
    "\n",
    "# wandb.init(project=\"Summarize_DP_PubMed\", entity=\"ivolinengong\")\n",
    "\n",
    "\n",
    "# # Check if CUDA is available and set the device\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # Set seeds for reproducibility\n",
    "# torch.manual_seed(42)\n",
    "\n",
    "# # Load the tokenizer\n",
    "# max_input_length = 150 #1024\n",
    "# max_output_length = 100 #256 #250\n",
    "# batch_size = 5\n",
    "# epochs = 8\n",
    "# gradient_accumulation_steps = 10\n",
    "\n",
    "# # Load pubmed dataset\n",
    "# # Load SAMSum dataset\n",
    "# train_dataset = load_dataset(\"samsum\", split=\"train\")\n",
    "# val_dataset = load_dataset(\"samsum\", split=\"validation\")\n",
    "\n",
    "# # tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
    "# # train_dataset = tokenized_datasets[\"train\")\n",
    "\n",
    "# # Checkpoint directories\n",
    "# # checkpoint_dir = \"./results/dp_results/test/run\"\n",
    "# checkpoint_dir = f\"./results/dp_results/{wandb.run.name}\"\n",
    "# # checkpoint_dir = f\"./results/dp_results/test/rich-serenity-9\"\n",
    "# os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# model_tokenizer_path = checkpoint_dir\n",
    "# optimizer_checkpoint_path = os.path.join(checkpoint_dir, \"optimizer_and_loss.pth\")\n",
    "\n",
    "# # Load model and tokenizer, and optimizer state if available\n",
    "# # model_checkpoint = find_latest_checkpoint(checkpoint_dir)\n",
    "# # if model_checkpoint:\n",
    "# #     print(f\"Resuming from checkpoint: {model_checkpoint}\")\n",
    "# #     # model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint).to(device)\n",
    "# #     # tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "# #     model = T5ForConditionalGeneration.from_pretrained(\"t5-large\").to(device)\n",
    "# #     tokenizer = T5Tokenizer.from_pretrained(\"t5-large\", model_max_length=max_input_length)\n",
    "\n",
    "# #     if os.path.isfile(optimizer_checkpoint_path):\n",
    "# #         print(\"Loading optimizer state and best loss from checkpoint\")\n",
    "# #         checkpoint = torch.load(optimizer_checkpoint_path, map_location=device)\n",
    "# #         optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "# #         optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "# #         best_loss = checkpoint['best_loss']\n",
    "# model_checkpoint = find_latest_checkpoint(checkpoint_dir)\n",
    "# if model_checkpoint:\n",
    "#     print(f\"Resuming from checkpoint: {model_checkpoint}\")\n",
    "\n",
    "#     # Load model and tokenizer from the checkpoint\n",
    "#     model = T5ForConditionalGeneration.from_pretrained(model_checkpoint).to(device)\n",
    "#     tokenizer = T5Tokenizer.from_pretrained(model_checkpoint, model_max_length=max_input_length)\n",
    "\n",
    "#     # Load optimizer state\n",
    "#     if os.path.isfile(optimizer_checkpoint_path):\n",
    "#         print(\"Loading optimizer state and best loss from checkpoint\")\n",
    "#         checkpoint = torch.load(optimizer_checkpoint_path, map_location=device)\n",
    "#         optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "#         optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "#         best_loss = checkpoint['best_loss']\n",
    "# else:\n",
    "#     print(\"Starting training from the pretrained model\")\n",
    "#     model = T5ForConditionalGeneration.from_pretrained(\"t5-large\").to(device)\n",
    "#     tokenizer = T5Tokenizer.from_pretrained(\"t5-large\", model_max_length=max_input_length)\n",
    "\n",
    "#     # model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-large\").to(device)\n",
    "#     # tokenizer = AutoTokenizer.from_pretrained(\"t5-large\", model_max_length=max_input_length)\n",
    "#     optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "#     best_loss = float('inf')\n",
    "\n",
    "# tokenized_train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "# tokenized_val_dataset = val_dataset.map(preprocess_function, batched=True)\n",
    "# tokenized_train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "# tokenized_val_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "\n",
    "# print(\"\\nTraining dataset size:\", len(tokenized_train_dataset))\n",
    "# print(\"Validation dataset size:\", len(tokenized_val_dataset))\n",
    "\n",
    "\n",
    "# # # Inspect a sample\n",
    "# sample = tokenized_train_dataset[0]\n",
    "# print(f\"Sample keys: {sample.keys()}\")\n",
    "# print(f\"Input IDs shape: {sample['input_ids'].shape}\")\n",
    "# print(f\"Attention mask shape: {sample['attention_mask'].shape}\")\n",
    "# print(f\"Labels shape: {sample['labels'].shape}\")\n",
    "# # Calculate and print average token length\n",
    "# # compute_average_length(tokenized_train_dataset)\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # Attach the privacy engine to the optimizer\n",
    "# privacy_engine = PrivacyEngine(\n",
    "#     model,\n",
    "#     batch_size=batch_size,\n",
    "#     sample_size=len(tokenized_train_dataset),\n",
    "#     epochs=epochs,\n",
    "#     max_grad_norm=1.0,\n",
    "#     target_epsilon=3,\n",
    "#     noise_multiplier=0.1,\n",
    "#     # clipping_mode=\"ghost\",\n",
    "# )\n",
    "# privacy_engine.attach(optimizer)\n",
    "\n",
    "# # Learning rate scheduler\n",
    "# total_steps = len(tokenized_train_dataset) // batch_size * epochs\n",
    "# scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "# # Data loader\n",
    "# data_loader = DataLoader(tokenized_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# val_data_loader = DataLoader(tokenized_val_dataset, batch_size=batch_size)\n",
    "\n",
    "\n",
    "# # Training loop\n",
    "# model.train()\n",
    "# for epoch in range(epochs):\n",
    "#     total_loss = 0\n",
    "#     print(f\"\\nStarting epoch {epoch + 1}/{epochs}\", flush=True)\n",
    "#     for i, batch in enumerate(data_loader):\n",
    "#         # Move batch to device\n",
    "#         input_ids = batch['input_ids'].to(device)\n",
    "#         attention_mask = batch['attention_mask'].to(device)\n",
    "#         labels = batch['labels'].to(device)\n",
    "\n",
    "#         # Forward pass\n",
    "#         outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "#         logits = outputs.logits\n",
    "\n",
    "#         # Compute per-example loss\n",
    "#         shift_logits = logits[..., :-1, :].contiguous()\n",
    "#         shift_labels = labels[..., 1:].contiguous()\n",
    "#         loss = F.cross_entropy(shift_logits.permute(0, 2, 1), shift_labels, reduction=\"none\").mean(dim=1)\n",
    "\n",
    "#         # Gradient accumulation\n",
    "#         if (i + 1) % gradient_accumulation_steps == 0:\n",
    "#             optimizer.step(loss=loss)  # Perform parameter update\n",
    "#             optimizer.zero_grad()  # Reset gradients\n",
    "#             scheduler.step()\n",
    "#         else:\n",
    "#             optimizer.virtual_step(loss=loss)  # Accumulate gradients\n",
    "\n",
    "\n",
    "#         # Save the model if this is the best loss so far\n",
    "#         avg_loss = torch.mean(loss).item()\n",
    "#         total_loss += avg_loss\n",
    "\n",
    "#         # Check if the current loss is the best\n",
    "#         if avg_loss < best_loss:\n",
    "#             best_loss = avg_loss\n",
    "\n",
    "#             # Save model and tokenizer using save_pretrained\n",
    "#             model.save_pretrained(checkpoint_dir)\n",
    "#             tokenizer.save_pretrained(checkpoint_dir)\n",
    "\n",
    "#             # Save optimizer state and best_loss separately\n",
    "#             torch.save({\n",
    "#                 'optimizer_state_dict': optimizer.state_dict(),\n",
    "#                 'best_loss': best_loss,\n",
    "#             }, optimizer_checkpoint_path)\n",
    "\n",
    "#             print(f\"New best model saved with loss: {best_loss}\")\n",
    "\n",
    "#         # # Log progress\n",
    "#         if i % 10 == 0:\n",
    "#             print(f\"Epoch: {epoch + 1}, Step: {i}/{len(data_loader)}, Loss: {avg_loss}\", flush=True)\n",
    "\n",
    "#     # Calculate the average training loss and perplexity for the epoch\n",
    "#     avg_train_loss = total_loss / len(data_loader)\n",
    "#     train_perplexity = np.exp(avg_train_loss)\n",
    "#     wandb.log({\"epoch\": epoch + 1, \"train_loss\": avg_train_loss, \"train_perplexity\": train_perplexity})\n",
    "\n",
    "# wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_envs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
